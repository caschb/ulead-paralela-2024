#+TITLE: Programación paralela distribuida con MPI
#+AUTHOR: Christian Asch
#+OPTIONS: toc:nil date:nil
#+LANGUAGE: spanish
#+LATEX_CLASS_OPTIONS: [a4paper,11pt]
#+LATEX_HEADER: \usepackage[margin=0.8in]{geometry}
#+LATEX_HEADER: \usepackage[spanish]{babel}

* Comunicaciones colectivas en MPI

** MPI_Barrier
#+begin_src C :tangle barrier.c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    MPI_Init(&argc, &argv);
    
    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    printf("Process %d before barrier\n", rank);
    
    // Synchronize all processes
    MPI_Barrier(MPI_COMM_WORLD);

    printf("Process %d after barrier\n", rank);

    MPI_Finalize();
    return 0;
}
#+end_src

** MPI_Bcast
#+begin_src C :tangle broadcast.c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    MPI_Init(&argc, &argv);
    
    int rank, value;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    if (rank == 0) {
        value = 42;  // Root process sets the value
    }

    // Broadcast value from process 0 to all processes
    MPI_Bcast(&value, 1, MPI_INT, 0, MPI_COMM_WORLD);

    printf("Process %d received value %d\n", rank, value);

    MPI_Finalize();
    return 0;
}
#+end_src

** MPI_Gather
#+begin_src C :tangle gather.c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    MPI_Init(&argc, &argv);
    
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int send_value = rank;  // Each process sends its rank
    int recv_values[size];  // Only the root process will receive values

    // Gather values at root process (rank 0)
    MPI_Gather(&send_value, 1, MPI_INT, recv_values, 1, MPI_INT, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Root process gathered values: ");
        for (int i = 0; i < size; i++) {
            printf("%d ", recv_values[i]);
        }
        printf("\n");
    }

    MPI_Finalize();
    return 0;
}

#+end_src

** MPI_Scatter
#+begin_src C :tangle scatter.c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    MPI_Init(&argc, &argv);
    
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int send_values[size];  // Only the root process will send values
    if (rank == 0) {
        for (int i = 0; i < size; i++) {
            send_values[i] = i * 10;
        }
    }

    int recv_value;  // Each process will receive one value

    // Scatter values from root process (rank 0) to all processes
    MPI_Scatter(send_values, 1, MPI_INT, &recv_value, 1, MPI_INT, 0, MPI_COMM_WORLD);

    printf("Process %d received value %d\n", rank, recv_value);

    MPI_Finalize();
    return 0;
}

#+end_src

** MPI_Allgather
#+begin_src C :tangle all_gather.c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    MPI_Init(&argc, &argv);
    
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int send_value = rank;  // Each process sends its rank
    int recv_values[size];  // All processes will receive all values

    // Gather values from all processes at all processes
    MPI_Allgather(&send_value, 1, MPI_INT, recv_values, 1, MPI_INT, MPI_COMM_WORLD);

    printf("Process %d received values: ", rank);
    for (int i = 0; i < size; i++) {
        printf("%d ", recv_values[i]);
    }
    printf("\n");

    MPI_Finalize();
    return 0;
}
#+end_src

** MPI_Alltoall
#+begin_src C :tangle all_to_all.c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    MPI_Init(&argc, &argv);
    
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    int send_values[size];  // Each process sends one value to each other process
    int recv_values[size];  // Each process receives one value from each other process

    for (int i = 0; i < size; i++) {
        send_values[i] = rank * 10 + i;
    }

    // All processes exchange data with all other processes
    MPI_Alltoall(send_values, 1, MPI_INT, recv_values, 1, MPI_INT, MPI_COMM_WORLD);

    printf("Process %d received values: ", rank);
    for (int i = 0; i < size; i++) {
        printf("%d ", recv_values[i]);
    }
    printf("\n");

    MPI_Finalize();
    return 0;
}
#+end_src

** MPI_Reduce
#+begin_src C :tangle reduce.c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    MPI_Init(&argc, &argv);
    
    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    int send_value = rank + 1;  // Each process sends its rank + 1
    int result;

    // Reduce values from all processes to the root process (rank 0)
    MPI_Reduce(&send_value, &result, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);

    if (rank == 0) {
        printf("Sum of ranks: %d\n", result);
    }

    MPI_Finalize();
    return 0;
}
#+end_src

** MPI_Allreduce
#+begin_src C :tangle all_reduce.c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char *argv[]) {
    MPI_Init(&argc, &argv);
    
    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    int send_value = rank + 1;  // Each process sends its rank + 1
    int result;

    // All processes get the result of reduction (e.g., sum)
    MPI_Allreduce(&send_value, &result, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);

    printf("Process %d sees the sum of ranks: %d\n", rank, result);

    MPI_Finalize();
    return 0;
}
#+end_src


** Makefile
#+begin_src makefile :tangle Makefile
CC=mpicc

%: %.c
	$(CC) ${^} -o ${@}
#+end_src


* Ejemplo con Python

** Solución simple
#+begin_src python :tangle filter.py
from mpi4py import MPI
from PIL import Image, ImageFilter
import sys

def apply_filter(image_part):
    return image_part.filter(ImageFilter.EMBOSS)

def divide_image(image, num_parts):
    width, height = image.size
    part_height = height // num_parts
    parts = []

    for i in range(num_parts):
        upper = i * part_height
        lower = (i + 1) * part_height if i != num_parts -1 else height
        part = image.crop((0, upper, width, lower))
        parts.append(part)
    return parts

def reassemble_image(parts, width, height):
    new_image = Image.new("RGB", (width, height))
    y_offset = 0
    for part in parts:
        new_image.paste(part, (0, y_offset))
        y_offset += part.size[1]
    return new_image

def main():
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    comm_size = comm.Get_size()

    if len(sys.argv) != 2:
        sys.exit(f"Usage: {sys.argv[0]} <name of image>")

    if rank == 0:
        image = Image.open(sys.argv[1])
        width, height = image.size
        image_parts = divide_image(image, comm_size)
    else:
        image_parts = None
        width = None
        height = None

    width = comm.bcast(width, root=0)
    height = comm.bcast(height, root=0)

    image_part = comm.scatter(image_parts, root=0)
    filtered_part = apply_filter(image_part)
    filtered_parts = comm.gather(filtered_part, root=0)

    if rank == 0:
        filtered_image = reassemble_image(filtered_parts, width, height)
        filtered_image.save(f"{sys.argv[1]}_filtered.png")

if __name__ == "__main__":
    main()
#+end_src

** Solución con fronteras
#+begin_src python :tangle filterv2.py
from mpi4py import MPI
from PIL import Image, ImageFilter
import numpy as np
import sys

def apply_filter(image_part):
    return image_part.filter(ImageFilter.EMBOSS)

def divide_image(image, num_parts, overlap):
    width, height = image.size
    part_height = height // num_parts
    parts = []
    
    for i in range(num_parts):
        upper = max(0, i * part_height - overlap)
        lower = min(height, (i + 1) * part_height + overlap)
        part = image.crop((0, upper, width, lower))
        parts.append((part, upper, lower))
    
    return parts

def reassemble_image(parts, width, height, overlap):
    new_image = Image.new('RGB', (width, height - overlap * len(parts)))
    y_offset = 0
    
    for part, original_upper, original_lower in parts:
        part_height = original_lower - original_upper
        cropped_part = part.crop((0, overlap + original_upper - original_upper, width, original_lower - original_upper - overlap))
        new_image.paste(cropped_part, (0, y_offset))
        y_offset += cropped_part.size[1]
    
    return new_image

def main():
    comm = MPI.COMM_WORLD
    rank = comm.Get_rank()
    size = comm.Get_size()
    overlap = 10

    if len(sys.argv) != 2:
        sys.exit(f"Usage: {sys.argv[0]} <name of image>")

    if rank == 0:
        image = Image.open(sys.argv[1])
        width, height = image.size

        image_parts = divide_image(image, size, overlap)
    else:
        image_parts = None
        width = None
        height = None

    width = comm.bcast(width, root=0)
    height = comm.bcast(height, root=0)

    image_part_info = comm.scatter(image_parts, root=0)
    image_part, upper, lower = image_part_info

    filtered_part = apply_filter(image_part)

    filtered_parts = comm.gather((filtered_part, upper, lower), root=0)

    if rank == 0:
        filtered_image = reassemble_image(filtered_parts, width, height)
        filtered_image.save(f"{sys.argv[1]}_filtered.png")

if __name__ == "__main__":
    main()
#+end_src

** Halo cells
#+begin_src python :tangle halo.py

#+end_src
